{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-28T18:34:49.761749Z",
     "start_time": "2025-12-28T18:34:46.791881Z"
    }
   },
   "source": [
    "# level92_dcc_connectedness.py\n",
    "# Level-92: Fast Time-Varying Connectedness via DCC-lite (EWMA + Precision Network)\n",
    "#\n",
    "# What it does:\n",
    "# - Downloads prices (robust to yfinance MultiIndex)\n",
    "# - Computes returns\n",
    "# - Builds time-varying covariance using EWMA (RiskMetrics-style) OR DCC-lite update\n",
    "# - Converts covariance -> correlation -> precision (inverse corr) -> partial correlations\n",
    "# - Uses partial-corr magnitudes to define a spillover/connectedness matrix\n",
    "# - Computes:\n",
    "#   - TCI (total connectedness index)\n",
    "#   - TO / FROM / NET per asset (time-varying)\n",
    "#\n",
    "# Outputs:\n",
    "# - level92_dcc_connectedness.csv\n",
    "# - level92_dcc_edges.csv\n",
    "# - level92_dcc_summary.json\n",
    "#\n",
    "# Run:\n",
    "#   python level92_dcc_connectedness.py\n",
    "#   python level92_dcc_connectedness.py --symbols SPY QQQ IWM EFA EEM TLT LQD GLD --start 2010-01-01\n",
    "#   python level92_dcc_connectedness.py --lambda 0.97 --min_obs 700 --edges_topk 30\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "# ----------------------------- Config -----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbols: Tuple[str, ...] = (\"SPY\", \"QQQ\", \"IWM\", \"EFA\", \"EEM\", \"TLT\", \"LQD\", \"GLD\")\n",
    "    start: str = \"2010-01-01\"\n",
    "\n",
    "    # EWMA decay (RiskMetrics). Higher => smoother.\n",
    "    lam: float = 0.97\n",
    "\n",
    "    # Use log returns by default\n",
    "    use_log_returns: bool = True\n",
    "\n",
    "    # Stability / speed controls\n",
    "    min_obs: int = 600              # need enough history before producing time-varying stats\n",
    "    ridge: float = 1e-6             # ridge added to correlation matrix before inversion\n",
    "    edges_topk: int = 40            # write only top-k edges per day (by weight)\n",
    "\n",
    "    seed: int = 42\n",
    "\n",
    "    out_conn_csv: str = \"level92_dcc_connectedness.csv\"\n",
    "    out_edges_csv: str = \"level92_dcc_edges.csv\"\n",
    "    out_json: str = \"level92_dcc_summary.json\"\n",
    "\n",
    "\n",
    "# ----------------------------- Robust yfinance loader -----------------------------\n",
    "def _extract_close_series(px: pd.DataFrame, symbol: str) -> pd.Series:\n",
    "    if px is None or px.empty:\n",
    "        raise RuntimeError(f\"No data for {symbol}\")\n",
    "\n",
    "    if isinstance(px.columns, pd.MultiIndex):\n",
    "        candidates = [\n",
    "            (\"Adj Close\", symbol),\n",
    "            (\"Close\", symbol),\n",
    "            (symbol, \"Adj Close\"),\n",
    "            (symbol, \"Close\"),\n",
    "        ]\n",
    "        for key in candidates:\n",
    "            if key in px.columns:\n",
    "                s = px[key].copy()\n",
    "                if isinstance(s, pd.DataFrame):\n",
    "                    s = s.iloc[:, 0]\n",
    "                s.name = symbol\n",
    "                return s\n",
    "\n",
    "        # fallback scan\n",
    "        cols = []\n",
    "        for c in px.columns:\n",
    "            c0 = str(c[0]).lower()\n",
    "            c1 = str(c[1]).lower()\n",
    "            if (symbol.lower() in c0 or symbol.lower() in c1) and (\"close\" in c0 or \"close\" in c1):\n",
    "                cols.append(c)\n",
    "        if cols:\n",
    "            s = px[cols[0]].copy()\n",
    "            if isinstance(s, pd.DataFrame):\n",
    "                s = s.iloc[:, 0]\n",
    "            s.name = symbol\n",
    "            return s\n",
    "\n",
    "        raise RuntimeError(f\"Could not extract Close/Adj Close for {symbol} from MultiIndex columns\")\n",
    "\n",
    "    for col in [\"Adj Close\", \"Close\"]:\n",
    "        if col in px.columns:\n",
    "            s = px[col].copy()\n",
    "            if isinstance(s, pd.DataFrame):\n",
    "                s = s.iloc[:, 0]\n",
    "            s.name = symbol\n",
    "            return s\n",
    "\n",
    "    raise RuntimeError(f\"Missing Close/Adj Close for {symbol}. Columns={list(px.columns)}\")\n",
    "\n",
    "\n",
    "def load_prices(symbols: Tuple[str, ...], start: str) -> pd.DataFrame:\n",
    "    symbols = tuple(symbols)\n",
    "\n",
    "    # try batch download first (faster)\n",
    "    try:\n",
    "        px_all = yf.download(list(symbols), start=start, progress=False, group_by=\"column\", auto_adjust=False)\n",
    "        if px_all is not None and not px_all.empty:\n",
    "            series_list = []\n",
    "            ok = True\n",
    "            for s in symbols:\n",
    "                try:\n",
    "                    series_list.append(_extract_close_series(px_all, s))\n",
    "                except Exception:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok and series_list:\n",
    "                prices = pd.concat(series_list, axis=1).sort_index().dropna(how=\"any\")\n",
    "                return prices\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback per symbol\n",
    "    frames: List[pd.Series] = []\n",
    "    for s in symbols:\n",
    "        px = yf.download(s, start=start, progress=False, auto_adjust=False)\n",
    "        frames.append(_extract_close_series(px, s))\n",
    "    prices = pd.concat(frames, axis=1).sort_index().dropna(how=\"any\")\n",
    "    return prices\n",
    "\n",
    "\n",
    "def compute_returns(prices: pd.DataFrame, use_log: bool) -> pd.DataFrame:\n",
    "    if use_log:\n",
    "        rets = np.log(prices).diff()\n",
    "    else:\n",
    "        rets = prices.pct_change()\n",
    "\n",
    "    rets = rets.replace([np.inf, -np.inf], np.nan)\n",
    "    rets = rets.dropna()\n",
    "    rets = rets.asfreq(\"B\").dropna()\n",
    "    return rets\n",
    "\n",
    "\n",
    "# ----------------------------- Math helpers -----------------------------\n",
    "def corr_from_cov(cov: np.ndarray) -> np.ndarray:\n",
    "    d = np.sqrt(np.maximum(np.diag(cov), 1e-18))\n",
    "    invd = 1.0 / d\n",
    "    corr = cov * invd[:, None] * invd[None, :]\n",
    "    corr = np.clip(corr, -0.999999, 0.999999)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "    return corr\n",
    "\n",
    "\n",
    "def partial_corr_from_corr(corr: np.ndarray, ridge: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Partial correlation from correlation matrix via precision matrix:\n",
    "      P = inv(C + ridge I)\n",
    "      rho_ij.partial = -P_ij / sqrt(P_ii * P_jj)\n",
    "    \"\"\"\n",
    "    n = corr.shape[0]\n",
    "    C = corr.copy()\n",
    "    C.flat[:: n + 1] += ridge  # add ridge to diagonal\n",
    "\n",
    "    P = np.linalg.inv(C)\n",
    "    denom = np.sqrt(np.outer(np.diag(P), np.diag(P)))\n",
    "    pc = -P / np.maximum(denom, 1e-18)\n",
    "    np.fill_diagonal(pc, 0.0)\n",
    "    pc = np.clip(pc, -0.999999, 0.999999)\n",
    "    return pc\n",
    "\n",
    "\n",
    "def connectedness_from_partial(pc: np.ndarray, labels: List[str]) -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Build a spillover matrix W from |partial correlations|:\n",
    "      w_ij = |pc_ij| / sum_k |pc_ik|   (row-normalized)\n",
    "    Then:\n",
    "      FROM_i = sum_j w_ij\n",
    "      TO_i   = sum_j w_ji\n",
    "      NET_i  = TO_i - FROM_i\n",
    "      TCI    = 100 * (sum offdiag W) / N\n",
    "    \"\"\"\n",
    "    W = np.abs(pc)\n",
    "    row_sums = W.sum(axis=1, keepdims=True)\n",
    "    row_sums = np.where(row_sums <= 0, 1.0, row_sums)\n",
    "    W = W / row_sums\n",
    "\n",
    "    N = W.shape[0]\n",
    "    off = W.copy()\n",
    "    np.fill_diagonal(off, 0.0)\n",
    "\n",
    "    FROM = off.sum(axis=1)\n",
    "    TO = off.sum(axis=0)\n",
    "    NET = TO - FROM\n",
    "    TCI = 100.0 * off.sum() / N\n",
    "\n",
    "    return {\n",
    "        \"W\": W,\n",
    "        \"TCI\": float(TCI),\n",
    "        \"FROM\": {labels[i]: float(FROM[i] * 100.0) for i in range(N)},\n",
    "        \"TO\": {labels[i]: float(TO[i] * 100.0) for i in range(N)},\n",
    "        \"NET\": {labels[i]: float(NET[i] * 100.0) for i in range(N)},\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------- Pipeline -----------------------------\n",
    "def run_pipeline(cfg: Config) -> Dict[str, object]:\n",
    "    np.random.seed(cfg.seed)\n",
    "\n",
    "    print(f\"[INFO] Downloading prices for {cfg.symbols} from {cfg.start} ...\")\n",
    "    prices = load_prices(cfg.symbols, cfg.start)\n",
    "    rets = compute_returns(prices, cfg.use_log_returns)\n",
    "    rets = rets.loc[:, list(cfg.symbols)].dropna(how=\"any\")\n",
    "\n",
    "    print(f\"[INFO] Got {len(prices)} price rows, {len(rets)} return rows, assets={rets.shape[1]}\")\n",
    "\n",
    "    X = rets.values\n",
    "    T, N = X.shape\n",
    "    labels = list(rets.columns)\n",
    "\n",
    "    if T < cfg.min_obs:\n",
    "        raise RuntimeError(f\"Not enough observations ({T}) for min_obs={cfg.min_obs}\")\n",
    "\n",
    "    # Initialize EWMA covariance with sample covariance on first min_obs block\n",
    "    init = np.cov(X[:cfg.min_obs].T, ddof=1)\n",
    "    S = init.copy()\n",
    "\n",
    "    rows = []\n",
    "    edges_rows = []\n",
    "\n",
    "    for t in range(cfg.min_obs, T):\n",
    "        x = X[t]\n",
    "        # EWMA covariance update: S_t = lam*S_{t-1} + (1-lam) * x x'\n",
    "        S = cfg.lam * S + (1.0 - cfg.lam) * np.outer(x, x)\n",
    "\n",
    "        corr = corr_from_cov(S)\n",
    "        pc = partial_corr_from_corr(corr, ridge=cfg.ridge)\n",
    "\n",
    "        conn = connectedness_from_partial(pc, labels)\n",
    "\n",
    "        dt = rets.index[t]\n",
    "        row = {\"date\": dt, \"TCI\": conn[\"TCI\"]}\n",
    "        for a in labels:\n",
    "            row[f\"TO_{a}\"] = conn[\"TO\"][a]\n",
    "            row[f\"FROM_{a}\"] = conn[\"FROM\"][a]\n",
    "            row[f\"NET_{a}\"] = conn[\"NET\"][a]\n",
    "        rows.append(row)\n",
    "\n",
    "        # edges: use W weights\n",
    "        W = conn[\"W\"]\n",
    "        # top-k directed edges by weight per day\n",
    "        if cfg.edges_topk > 0:\n",
    "            flat = []\n",
    "            for i, src in enumerate(labels):\n",
    "                for j, dst in enumerate(labels):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    flat.append((src, dst, float(W[i, j] * 100.0)))\n",
    "            flat.sort(key=lambda z: z[2], reverse=True)\n",
    "            for (src, dst, w) in flat[:cfg.edges_topk]:\n",
    "                edges_rows.append({\"date\": dt, \"src\": src, \"dst\": dst, \"weight_pct\": w})\n",
    "\n",
    "        if (t - cfg.min_obs) % max(1, ((T - cfg.min_obs) // 10)) == 0:\n",
    "            print(f\"[INFO] Progress {t - cfg.min_obs}/{T - cfg.min_obs} ...\")\n",
    "\n",
    "    conn_df = pd.DataFrame(rows).set_index(\"date\").sort_index()\n",
    "    edges_df = pd.DataFrame(edges_rows) if edges_rows else pd.DataFrame(columns=[\"date\", \"src\", \"dst\", \"weight_pct\"])\n",
    "\n",
    "    summary = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"data_window\": {\n",
    "            \"start\": str(rets.index.min().date()),\n",
    "            \"end\": str(rets.index.max().date()),\n",
    "            \"n_returns\": int(len(rets)),\n",
    "            \"assets\": int(N),\n",
    "        },\n",
    "        \"connectedness\": {\n",
    "            \"n_points\": int(len(conn_df)),\n",
    "            \"tci_min\": float(conn_df[\"TCI\"].min()),\n",
    "            \"tci_max\": float(conn_df[\"TCI\"].max()),\n",
    "            \"tci_last\": float(conn_df[\"TCI\"].iloc[-1]),\n",
    "            \"date_first\": str(conn_df.index.min().date()),\n",
    "            \"date_last\": str(conn_df.index.max().date()),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return {\"conn\": conn_df, \"edges\": edges_df, \"summary\": summary}\n",
    "\n",
    "\n",
    "def save_outputs(result: Dict[str, object], cfg: Config) -> None:\n",
    "    conn: pd.DataFrame = result[\"conn\"]  # type: ignore\n",
    "    edges: pd.DataFrame = result[\"edges\"]  # type: ignore\n",
    "    summary: Dict = result[\"summary\"]  # type: ignore\n",
    "\n",
    "    os.makedirs(os.path.dirname(cfg.out_conn_csv) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(cfg.out_edges_csv) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(cfg.out_json) or \".\", exist_ok=True)\n",
    "\n",
    "    conn.to_csv(cfg.out_conn_csv)\n",
    "    edges.to_csv(cfg.out_edges_csv, index=False)\n",
    "    with open(cfg.out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Saved connectedness → {cfg.out_conn_csv}\")\n",
    "    print(f\"[OK] Saved edges → {cfg.out_edges_csv}\")\n",
    "    print(f\"[OK] Saved summary → {cfg.out_json}\")\n",
    "\n",
    "    # print last snapshot\n",
    "    last = conn.iloc[-1]\n",
    "    labels = [c.replace(\"NET_\", \"\") for c in conn.columns if c.startswith(\"NET_\")]\n",
    "    net = {a: float(last[f\"NET_{a}\"]) for a in labels}\n",
    "    top_tx = sorted(net.items(), key=lambda kv: kv[1], reverse=True)[:5]\n",
    "    top_rx = sorted(net.items(), key=lambda kv: kv[1])[:5]\n",
    "    print(f\"[LAST] TCI={float(last['TCI']):.2f}\")\n",
    "    print(\"[LAST] Top transmitters (NET high):\", top_tx)\n",
    "    print(\"[LAST] Top receivers (NET low):\", top_rx)\n",
    "\n",
    "\n",
    "# ----------------------------- CLI -----------------------------\n",
    "def parse_args() -> Config:\n",
    "    p = argparse.ArgumentParser(description=\"Level-92: Fast Time-Varying Connectedness via EWMA+DCC-lite partial correlations\")\n",
    "\n",
    "    p.add_argument(\"--start\", type=str, default=Config.start)\n",
    "    p.add_argument(\"--symbols\", nargs=\"+\", default=list(Config.symbols))\n",
    "\n",
    "    p.add_argument(\"--lambda\", dest=\"lam\", type=float, default=Config.lam)\n",
    "    p.add_argument(\"--min_obs\", type=int, default=Config.min_obs)\n",
    "    p.add_argument(\"--ridge\", type=float, default=Config.ridge)\n",
    "    p.add_argument(\"--edges_topk\", type=int, default=Config.edges_topk)\n",
    "\n",
    "    p.add_argument(\"--simple-returns\", action=\"store_true\")\n",
    "\n",
    "    p.add_argument(\"--seed\", type=int, default=Config.seed)\n",
    "\n",
    "    p.add_argument(\"--conn-csv\", type=str, default=Config.out_conn_csv)\n",
    "    p.add_argument(\"--edges-csv\", type=str, default=Config.out_edges_csv)\n",
    "    p.add_argument(\"--json\", type=str, default=Config.out_json)\n",
    "\n",
    "    a = p.parse_args()\n",
    "    return Config(\n",
    "        symbols=tuple(a.symbols),\n",
    "        start=a.start,\n",
    "        lam=float(a.lam),\n",
    "        use_log_returns=(not a.simple_returns),\n",
    "        min_obs=int(a.min_obs),\n",
    "        ridge=float(a.ridge),\n",
    "        edges_topk=int(a.edges_topk),\n",
    "        seed=int(a.seed),\n",
    "        out_conn_csv=a.conn_csv,\n",
    "        out_edges_csv=a.edges_csv,\n",
    "        out_json=a.json,\n",
    "    )\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    cfg = parse_args()\n",
    "    result = run_pipeline(cfg)\n",
    "    save_outputs(result, cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter/PyCharm shim: strip \"-f kernel.json\" etc.\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]] + [\n",
    "        arg for arg in sys.argv[1:]\n",
    "        if arg != \"-f\" and not (arg.endswith(\".json\") and \"kernel\" in arg)\n",
    "    ]\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading prices for ('SPY', 'QQQ', 'IWM', 'EFA', 'EEM', 'TLT', 'LQD', 'GLD') from 2010-01-01 ...\n",
      "[INFO] Got 4021 price rows, 4020 return rows, assets=8\n",
      "[INFO] Progress 0/3420 ...\n",
      "[INFO] Progress 342/3420 ...\n",
      "[INFO] Progress 684/3420 ...\n",
      "[INFO] Progress 1026/3420 ...\n",
      "[INFO] Progress 1368/3420 ...\n",
      "[INFO] Progress 1710/3420 ...\n",
      "[INFO] Progress 2052/3420 ...\n",
      "[INFO] Progress 2394/3420 ...\n",
      "[INFO] Progress 2736/3420 ...\n",
      "[INFO] Progress 3078/3420 ...\n",
      "[OK] Saved connectedness → level92_dcc_connectedness.csv\n",
      "[OK] Saved edges → level92_dcc_edges.csv\n",
      "[OK] Saved summary → level92_dcc_summary.json\n",
      "[LAST] TCI=100.00\n",
      "[LAST] Top transmitters (NET high): [('QQQ', 47.71229184830401), ('SPY', 34.01644451325738), ('EEM', 22.808993291341185), ('LQD', 3.428428844136655), ('TLT', -0.8144486935771633)]\n",
      "[LAST] Top receivers (NET low): [('GLD', -54.11903809640679), ('IWM', -31.459289060949512), ('EFA', -21.57338264610571), ('TLT', -0.8144486935771633), ('LQD', 3.428428844136655)]\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
