{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-29T02:38:32.661878Z",
     "start_time": "2025-12-29T02:38:27.107298Z"
    }
   },
   "source": [
    "# level100_evt_es_risk_budget.py\n",
    "# Level-100: End-to-End Risk Engine — EVT (POT-GPD) Expected Shortfall Risk Budgeting\n",
    "#            + Monthly Rebalancing + Turnover/TC + Walk-Forward Backtest (SciPy-free)\n",
    "#\n",
    "# What this does (fast, stable, no long loops):\n",
    "# 1) Download prices (yfinance) for a small ETF universe\n",
    "# 2) Compute daily returns\n",
    "# 3) Each month, estimate each asset's *tail risk* using EVT POT-GPD ES on LOSSES:\n",
    "#       loss = -return  (right tail = big losses)\n",
    "#       threshold u = quantile(loss, q_u)\n",
    "#       exceed = loss-u | loss>u\n",
    "#       Fit GPD params (xi, beta) with Method-of-Moments (very fast, no optimizers)\n",
    "#       ES_loss(alpha) from POT formula\n",
    "#    If exceedances are too few, fallback to empirical ES (mean of worst alpha losses).\n",
    "# 4) Build risk-budget weights:\n",
    "#       w_i ∝ 1 / ES_loss_i\n",
    "#    with caps/floors and optional correlation shrink (lightweight)\n",
    "# 5) Hold weights until next rebalance date; subtract transaction costs at rebalances\n",
    "# 6) Save daily series + metrics JSON\n",
    "#\n",
    "# Outputs:\n",
    "#   - level100_evt_es_rb_daily.csv\n",
    "#   - level100_evt_es_rb_weights.csv\n",
    "#   - level100_evt_es_rb_summary.json\n",
    "#\n",
    "# Run:\n",
    "#   python level100_evt_es_risk_budget.py\n",
    "#   python level100_evt_es_risk_budget.py --alpha 0.01 --q_u 0.95 --window 1260 --rebalance ME\n",
    "#   python level100_evt_es_risk_budget.py --symbols SPY QQQ IWM EFA EEM TLT LQD GLD --tc_bps 2.0\n",
    "#\n",
    "# Notes:\n",
    "# - Uses pandas resample(\"ME\") to avoid the deprecated \"M\" warning.\n",
    "# - Avoids SciPy/statsmodels/cvxpy. Uses only numpy/pandas/yfinance.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "# ----------------------------- Config -----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbols: Tuple[str, ...] = (\"SPY\", \"QQQ\", \"IWM\", \"EFA\", \"EEM\", \"TLT\", \"LQD\", \"GLD\")\n",
    "    start: str = \"2010-01-01\"\n",
    "\n",
    "    use_log_returns: bool = True\n",
    "    dropna: bool = True\n",
    "\n",
    "    # EVT tail settings (on losses)\n",
    "    alpha: float = 0.05          # tail probability (ES at alpha)\n",
    "    q_u: float = 0.90            # threshold quantile for POT on losses (0.90 to 0.95 common)\n",
    "    window: int = 1000           # rolling window length (trading days)\n",
    "    min_exc: int = 40            # minimum exceedances required for POT fit\n",
    "    xi_clip: Tuple[float, float] = (-0.45, 0.45)  # stabilize MoM\n",
    "\n",
    "    # Rebalancing\n",
    "    rebalance: str = \"ME\"        # pandas offset alias, use \"ME\" (month-end), \"W-FRI\", etc.\n",
    "    max_w: float = 0.35          # per-asset cap\n",
    "    min_w: float = 0.00          # per-asset floor (0 for long-only)\n",
    "    allow_cash: bool = False     # if True, residual weight goes to CASH (0% return)\n",
    "    cash_symbol: str = \"CASH\"\n",
    "\n",
    "    # Simple correlation-aware scaling (optional)\n",
    "    use_corr_scale: bool = True\n",
    "    corr_shrink: float = 0.15    # shrink corr toward identity to reduce noise\n",
    "\n",
    "    # Transaction costs\n",
    "    tc_bps: float = 1.0          # cost per 100% turnover at rebalance (bps)\n",
    "                                 # cost applied: cost = tc_bps/1e4 * turnover\n",
    "\n",
    "    seed: int = 42\n",
    "\n",
    "    out_daily_csv: str = \"level100_evt_es_rb_daily.csv\"\n",
    "    out_weights_csv: str = \"level100_evt_es_rb_weights.csv\"\n",
    "    out_json: str = \"level100_evt_es_rb_summary.json\"\n",
    "\n",
    "\n",
    "# ----------------------------- yfinance loader-----------------------------\n",
    "def _extract_close(px: pd.DataFrame, symbol: str) -> pd.Series:\n",
    "    if px is None or px.empty:\n",
    "        raise RuntimeError(f\"No data returned for {symbol}\")\n",
    "\n",
    "    if isinstance(px.columns, pd.MultiIndex):\n",
    "        # yfinance often returns (PriceField, Symbol)\n",
    "        for key in [(\"Adj Close\", symbol), (\"Close\", symbol), (symbol, \"Adj Close\"), (symbol, \"Close\")]:\n",
    "            if key in px.columns:\n",
    "                s = px[key].copy()\n",
    "                if isinstance(s, pd.DataFrame):\n",
    "                    s = s.iloc[:, 0]\n",
    "                s.name = symbol\n",
    "                return s\n",
    "        raise RuntimeError(f\"Could not extract Close/Adj Close for {symbol} from MultiIndex columns.\")\n",
    "\n",
    "    for col in [\"Adj Close\", \"Close\"]:\n",
    "        if col in px.columns:\n",
    "            s = px[col].copy()\n",
    "            if isinstance(s, pd.DataFrame):\n",
    "                s = s.iloc[:, 0]\n",
    "            s.name = symbol\n",
    "            return s\n",
    "\n",
    "    raise RuntimeError(f\"Missing Close/Adj Close for {symbol}. Columns={list(px.columns)}\")\n",
    "\n",
    "\n",
    "def load_prices(symbols: Tuple[str, ...], start: str) -> pd.DataFrame:\n",
    "    syms = tuple(symbols)\n",
    "\n",
    "    # Try batch first (faster)\n",
    "    try:\n",
    "        px_all = yf.download(list(syms), start=start, progress=False, group_by=\"column\", auto_adjust=False)\n",
    "        if px_all is not None and not px_all.empty:\n",
    "            series = []\n",
    "            ok = True\n",
    "            for s in syms:\n",
    "                try:\n",
    "                    series.append(_extract_close(px_all, s))\n",
    "                except Exception:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok and series:\n",
    "                return pd.concat(series, axis=1).sort_index()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback single download per ticker\n",
    "    series = []\n",
    "    for s in syms:\n",
    "        px = yf.download(s, start=start, progress=False, auto_adjust=False)\n",
    "        series.append(_extract_close(px, s))\n",
    "    return pd.concat(series, axis=1).sort_index()\n",
    "\n",
    "\n",
    "def compute_returns(prices: pd.DataFrame, use_log: bool) -> pd.DataFrame:\n",
    "    prices = prices.replace([np.inf, -np.inf], np.nan)\n",
    "    rets = (np.log(prices).diff() if use_log else prices.pct_change())\n",
    "    rets = rets.replace([np.inf, -np.inf], np.nan)\n",
    "    return rets.dropna(how=\"all\")\n",
    "\n",
    "\n",
    "# ----------------------------- EVT POT-GPD (fast MoM) -----------------------------\n",
    "def gpd_mom_fit(exceed: np.ndarray, xi_clip: Tuple[float, float]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Method-of-moments fit for GPD on exceedances y>0.\n",
    "      mean = beta/(1-xi)\n",
    "      var  = beta^2/((1-xi)^2*(1-2xi))\n",
    "    => var/mean^2 = 1/(1-2xi)  => xi = 0.5*(1 - mean^2/var)\n",
    "       beta = mean*(1-xi)\n",
    "    \"\"\"\n",
    "    y = exceed.astype(float)\n",
    "    m = float(y.mean())\n",
    "    v = float(y.var(ddof=1)) if y.size >= 2 else float(y.var())\n",
    "    v = max(v, 1e-12)\n",
    "\n",
    "    xi = 0.5 * (1.0 - (m * m) / v)\n",
    "    xi = float(np.clip(xi, xi_clip[0], xi_clip[1]))\n",
    "\n",
    "    beta = m * (1.0 - xi)\n",
    "    beta = float(max(beta, 1e-12))\n",
    "    return xi, beta\n",
    "\n",
    "\n",
    "def pot_es_loss(losses: np.ndarray, alpha: float, q_u: float, min_exc: int, xi_clip: Tuple[float, float]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Estimate ES on losses using POT-GPD. Returns dict with:\n",
    "      es_loss, var_loss, u, k, method (pot/empirical)\n",
    "    If exceedances insufficient or alpha not in tail beyond u, fall back to empirical ES.\n",
    "    \"\"\"\n",
    "    L = losses.astype(float)\n",
    "    L = L[np.isfinite(L)]\n",
    "    n = int(L.size)\n",
    "    if n < 10:\n",
    "        return {\"es_loss\": float(\"nan\"), \"var_loss\": float(\"nan\"), \"u\": float(\"nan\"), \"k\": 0.0, \"method\": \"na\"}\n",
    "\n",
    "    # POT threshold\n",
    "    u = float(np.quantile(L, q_u))\n",
    "    exc = L[L > u] - u\n",
    "    k = int(exc.size)\n",
    "\n",
    "    # Empirical ES (always available if enough points)\n",
    "    q_alpha = float(np.quantile(L, 1.0 - alpha))  # alpha tail on losses is right tail => quantile at 1-alpha\n",
    "    tail = L[L >= q_alpha]\n",
    "    emp_es = float(tail.mean()) if tail.size > 0 else float(\"nan\")\n",
    "    emp_var = q_alpha\n",
    "\n",
    "    if k < min_exc:\n",
    "        return {\"es_loss\": emp_es, \"var_loss\": emp_var, \"u\": u, \"k\": float(k), \"method\": \"empirical\"}\n",
    "\n",
    "    p_u = k / n  # P(L>u)\n",
    "    if alpha >= p_u:\n",
    "        # desired tail prob not beyond threshold region\n",
    "        return {\"es_loss\": emp_es, \"var_loss\": emp_var, \"u\": u, \"k\": float(k), \"method\": \"empirical\"}\n",
    "\n",
    "    xi, beta = gpd_mom_fit(exc, xi_clip=xi_clip)\n",
    "\n",
    "    # VaR for losses where P(L>VaR)=alpha\n",
    "    if abs(xi) < 1e-8:\n",
    "        varL = u + beta * math.log(p_u / alpha)\n",
    "    else:\n",
    "        varL = u + (beta / xi) * (((alpha / p_u) ** (-xi)) - 1.0)\n",
    "\n",
    "    # ES for losses\n",
    "    if xi >= 0.999:\n",
    "        esL = emp_es\n",
    "        method = \"empirical\"\n",
    "    else:\n",
    "        yq = varL - u\n",
    "        esL = varL + (beta + xi * yq) / (1.0 - xi)\n",
    "        method = \"pot\"\n",
    "\n",
    "    return {\"es_loss\": float(esL), \"var_loss\": float(varL), \"u\": u, \"k\": float(k), \"method\": method}\n",
    "\n",
    "\n",
    "# ----------------------------- Portfolio mechanics -----------------------------\n",
    "def shrink_corr(corr: np.ndarray, shrink: float) -> np.ndarray:\n",
    "    n = corr.shape[0]\n",
    "    I = np.eye(n)\n",
    "    s = float(np.clip(shrink, 0.0, 1.0))\n",
    "    out = (1.0 - s) * corr + s * I\n",
    "    # clamp diagonal\n",
    "    np.fill_diagonal(out, 1.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def corr_scale_for_weights(w: np.ndarray, corr: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Simple scalar that penalizes highly correlated portfolios.\n",
    "    scale = 1/sqrt(w' C w) where C is correlation. Keeps weights direction,\n",
    "    just reduces exposure when correlation concentrates risk.\n",
    "    \"\"\"\n",
    "    v = float(w @ corr @ w)\n",
    "    v = max(v, 1e-12)\n",
    "    return 1.0 / math.sqrt(v)\n",
    "\n",
    "\n",
    "def clip_and_renorm(w: np.ndarray, lo: float, hi: float) -> np.ndarray:\n",
    "    w = np.clip(w, lo, hi)\n",
    "    s = float(w.sum())\n",
    "    if s <= 0:\n",
    "        return np.ones_like(w) / len(w)\n",
    "    return w / s\n",
    "\n",
    "\n",
    "def perf_stats(r: np.ndarray) -> Dict[str, float]:\n",
    "    r = r[np.isfinite(r)]\n",
    "    if r.size == 0:\n",
    "        return {\"ann_ret\": float(\"nan\"), \"ann_vol\": float(\"nan\"), \"sharpe\": float(\"nan\"), \"max_dd\": float(\"nan\")}\n",
    "    ann_ret = float(r.mean() * 252.0)\n",
    "    ann_vol = float(r.std(ddof=1) * math.sqrt(252.0))\n",
    "    sharpe = float(ann_ret / ann_vol) if ann_vol > 0 else float(\"nan\")\n",
    "    eq = np.cumprod(1.0 + r)\n",
    "    peak = np.maximum.accumulate(eq)\n",
    "    dd = (eq / peak) - 1.0\n",
    "    max_dd = float(dd.min()) if dd.size else float(\"nan\")\n",
    "    return {\"ann_ret\": ann_ret, \"ann_vol\": ann_vol, \"sharpe\": sharpe, \"max_dd\": max_dd}\n",
    "\n",
    "\n",
    "# ----------------------------- Pipeline -----------------------------\n",
    "def run_pipeline(cfg: Config) -> Dict[str, object]:\n",
    "    np.random.seed(cfg.seed)\n",
    "\n",
    "    print(f\"[INFO] Downloading prices for {cfg.symbols} from {cfg.start} ...\")\n",
    "    prices = load_prices(cfg.symbols, cfg.start)\n",
    "    rets = compute_returns(prices, cfg.use_log_returns)\n",
    "\n",
    "    if cfg.dropna:\n",
    "        rets = rets.dropna(how=\"any\")\n",
    "    if rets.empty:\n",
    "        raise RuntimeError(\"No returns after cleaning (check tickers/start).\")\n",
    "\n",
    "    dates = rets.index\n",
    "    n, m = rets.shape\n",
    "    if n <= cfg.window + 5:\n",
    "        raise RuntimeError(f\"Not enough rows for window={cfg.window}. rows={n}\")\n",
    "\n",
    "    # Rebalance dates (month-end, etc.)\n",
    "    # Using \"ME\" avoids the deprecated \"M\".\n",
    "    rebal_dates = rets.resample(cfg.rebalance).last().index\n",
    "    rebal_dates = rebal_dates[rebal_dates >= dates[cfg.window]]\n",
    "\n",
    "    print(f\"[INFO] rows={n}, assets={m}, rebalances={len(rebal_dates)} ({cfg.rebalance}), window={cfg.window}\")\n",
    "    print(f\"[INFO] EVT alpha={cfg.alpha}, q_u={cfg.q_u}, min_exc={cfg.min_exc}, tc_bps={cfg.tc_bps}\")\n",
    "\n",
    "    # Storage\n",
    "    w_hist = pd.DataFrame(index=rebal_dates, columns=list(cfg.symbols), dtype=float)\n",
    "    es_hist = pd.DataFrame(index=rebal_dates, columns=[f\"ESloss_{s}\" for s in cfg.symbols], dtype=float)\n",
    "    method_hist = pd.DataFrame(index=rebal_dates, columns=[f\"method_{s}\" for s in cfg.symbols], dtype=str)\n",
    "    exc_hist = pd.DataFrame(index=rebal_dates, columns=[f\"k_{s}\" for s in cfg.symbols], dtype=float)\n",
    "\n",
    "    # Daily holdings\n",
    "    w_daily = pd.DataFrame(index=dates, columns=list(cfg.symbols), dtype=float)\n",
    "\n",
    "    prev_w = np.zeros(m, dtype=float)\n",
    "    prev_w[:] = 1.0 / m  # start equal-weight\n",
    "\n",
    "    rets_np = rets.values\n",
    "\n",
    "    for dt in rebal_dates:\n",
    "        t = int(np.searchsorted(dates.values, dt.to_datetime64()))\n",
    "        if t <= cfg.window:\n",
    "            continue\n",
    "        win = rets_np[t - cfg.window:t, :]  # window x assets\n",
    "\n",
    "        # Losses per asset\n",
    "        losses = -win  # right tail\n",
    "\n",
    "        es = np.full(m, np.nan, dtype=float)\n",
    "        for j, s in enumerate(cfg.symbols):\n",
    "            res = pot_es_loss(\n",
    "                losses=losses[:, j],\n",
    "                alpha=cfg.alpha,\n",
    "                q_u=cfg.q_u,\n",
    "                min_exc=cfg.min_exc,\n",
    "                xi_clip=cfg.xi_clip,\n",
    "            )\n",
    "            es[j] = res[\"es_loss\"]\n",
    "            es_hist.loc[dt, f\"ESloss_{s}\"] = res[\"es_loss\"]\n",
    "            method_hist.loc[dt, f\"method_{s}\"] = res[\"method\"]\n",
    "            exc_hist.loc[dt, f\"k_{s}\"] = res[\"k\"]\n",
    "\n",
    "        # Risk budgeting: w ∝ 1/ES_loss  (ES_loss is positive-ish)\n",
    "        es_safe = np.where(np.isfinite(es) & (es > 1e-12), es, np.nan)\n",
    "        if np.all(~np.isfinite(es_safe)):\n",
    "            raw = np.ones(m) / m\n",
    "        else:\n",
    "            inv = 1.0 / np.where(np.isfinite(es_safe), es_safe, np.nan)\n",
    "            inv = np.where(np.isfinite(inv), inv, 0.0)\n",
    "            if inv.sum() <= 0:\n",
    "                raw = np.ones(m) / m\n",
    "            else:\n",
    "                raw = inv / inv.sum()\n",
    "\n",
    "        # Cap/floor\n",
    "        w = clip_and_renorm(raw, cfg.min_w, cfg.max_w)\n",
    "\n",
    "        # Optional corr-based scale (scalar, keeps weights long-only and fast)\n",
    "        if cfg.use_corr_scale:\n",
    "            C = np.corrcoef(win, rowvar=False)\n",
    "            C = np.nan_to_num(C, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            C = shrink_corr(C, cfg.corr_shrink)\n",
    "            scale = corr_scale_for_weights(w, C)\n",
    "            # We keep weights normalized (sum=1) for long-only.\n",
    "            # Scale is reported (can be used if you later add leverage targeting).\n",
    "            # For now we just store it in weights file as 'leverage_hint'.\n",
    "            leverage_hint = float(scale)\n",
    "        else:\n",
    "            leverage_hint = float(\"nan\")\n",
    "\n",
    "        # Save rebalance weights + hint\n",
    "        w_hist.loc[dt, :] = w\n",
    "        w_hist.loc[dt, :] = w_hist.loc[dt, :].astype(float)\n",
    "\n",
    "        # Assign weights forward until next rebalance date\n",
    "        # We'll fill daily after loop using merge-asof style\n",
    "        # but easiest: mark only at dt; later forward-fill.\n",
    "        w_daily.loc[dt, :] = w\n",
    "\n",
    "        prev_w = w\n",
    "\n",
    "    # Forward-fill daily weights; then backfill initial portion with first available weights\n",
    "    w_daily = w_daily.sort_index().ffill()\n",
    "    first_valid = w_daily.dropna(how=\"any\").index.min()\n",
    "    if first_valid is None:\n",
    "        raise RuntimeError(\"No rebalances produced weights (try smaller window or different rebalance freq).\")\n",
    "    w_daily.loc[:first_valid, :] = w_daily.loc[first_valid, :].values\n",
    "\n",
    "    # Compute daily portfolio returns with transaction costs at rebalance points\n",
    "    port = np.sum(rets_np * w_daily.values, axis=1)\n",
    "\n",
    "    # Turnover at rebalance dates (L1)\n",
    "    rebal_mask = np.zeros(n, dtype=bool)\n",
    "    rebal_mask[np.searchsorted(dates.values, rebal_dates.to_datetime64())] = True\n",
    "    # Compute turnover on those dates using change in weights from previous day\n",
    "    w_vals = w_daily.values\n",
    "    w_prev = np.vstack([w_vals[0:1, :], w_vals[:-1, :]])\n",
    "    turnover = np.sum(np.abs(w_vals - w_prev), axis=1)\n",
    "\n",
    "    tc = (cfg.tc_bps / 1e4) * turnover * rebal_mask.astype(float)\n",
    "    port_net = port - tc\n",
    "\n",
    "    # Build output daily df\n",
    "    daily = pd.DataFrame(index=dates)\n",
    "    daily[\"port_ret_gross\"] = port\n",
    "    daily[\"port_ret_net\"] = port_net\n",
    "    daily[\"turnover\"] = turnover\n",
    "    daily[\"tc_cost\"] = tc\n",
    "    for j, s in enumerate(cfg.symbols):\n",
    "        daily[f\"w_{s}\"] = w_daily.iloc[:, j].values\n",
    "        daily[f\"ret_{s}\"] = rets.iloc[:, j].values\n",
    "\n",
    "    # Summary stats\n",
    "    stats_gross = perf_stats(daily[\"port_ret_gross\"].values)\n",
    "    stats_net = perf_stats(daily[\"port_ret_net\"].values)\n",
    "\n",
    "    avg_turn = float(np.mean(turnover[rebal_mask])) if rebal_mask.any() else float(\"nan\")\n",
    "    med_turn = float(np.median(turnover[rebal_mask])) if rebal_mask.any() else float(\"nan\")\n",
    "\n",
    "    # Basic VaR exceptions check using rolling empirical VaR of portfolio net (quick sanity)\n",
    "    # (This is not the EVT VaR; just a quick backtest metric.)\n",
    "    r_net = daily[\"port_ret_net\"].values\n",
    "    var_roll = np.full(n, np.nan)\n",
    "    for t in range(cfg.window, n):\n",
    "        winp = r_net[t - cfg.window:t]\n",
    "        var_roll[t] = float(np.quantile(winp, cfg.alpha))\n",
    "    exc = np.isfinite(var_roll) & (r_net <= var_roll)\n",
    "    exc_rate = float(exc.sum() / np.isfinite(var_roll).sum()) if np.isfinite(var_roll).sum() > 0 else float(\"nan\")\n",
    "\n",
    "    summary = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"data_window\": {\"start\": str(dates.min().date()), \"end\": str(dates.max().date()), \"n_returns\": int(n)},\n",
    "        \"rebalancing\": {\"freq\": cfg.rebalance, \"n_rebalances\": int(len(rebal_dates))},\n",
    "        \"performance_gross\": stats_gross,\n",
    "        \"performance_net\": stats_net,\n",
    "        \"turnover\": {\"avg_rebalance_turnover_L1\": avg_turn, \"median_rebalance_turnover_L1\": med_turn},\n",
    "        \"quick_var_backtest\": {\n",
    "            \"alpha\": float(cfg.alpha),\n",
    "            \"exception_rate_vs_empirical_rolling_var\": exc_rate,\n",
    "            \"note\": \"This is a quick sanity check using rolling empirical VaR on portfolio net returns.\"\n",
    "        },\n",
    "        \"notes\": [\n",
    "            \"Weights are long-only and sum to 1 (no leverage).\",\n",
    "            \"Tail-risk model uses EVT POT-GPD ES on losses per asset; falls back to empirical ES when exceedances are too few.\",\n",
    "            \"This is designed to be fast and avoid long numerical loops or SciPy dependencies.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Add leverage hints as a separate column in weights output (optional diagnostics)\n",
    "    w_out = w_hist.copy()\n",
    "    w_out[\"leverage_hint\"] = np.nan\n",
    "    if cfg.use_corr_scale:\n",
    "        for dt in w_out.index:\n",
    "            t = int(np.searchsorted(dates.values, dt.to_datetime64()))\n",
    "            win = rets_np[t - cfg.window:t, :]\n",
    "            C = np.corrcoef(win, rowvar=False)\n",
    "            C = np.nan_to_num(C, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            C = shrink_corr(C, cfg.corr_shrink)\n",
    "            w = w_out.loc[dt, list(cfg.symbols)].values.astype(float)\n",
    "            w_out.loc[dt, \"leverage_hint\"] = corr_scale_for_weights(w, C)\n",
    "\n",
    "    return {\n",
    "        \"daily\": daily,\n",
    "        \"weights\": w_out,\n",
    "        \"es\": es_hist,\n",
    "        \"methods\": method_hist,\n",
    "        \"exceed\": exc_hist,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_outputs(res: Dict[str, object], cfg: Config) -> None:\n",
    "    daily: pd.DataFrame = res[\"daily\"]  # type: ignore\n",
    "    weights: pd.DataFrame = res[\"weights\"]  # type: ignore\n",
    "    es: pd.DataFrame = res[\"es\"]  # type: ignore\n",
    "    methods: pd.DataFrame = res[\"methods\"]  # type: ignore\n",
    "    exceed: pd.DataFrame = res[\"exceed\"]  # type: ignore\n",
    "    summary: Dict = res[\"summary\"]  # type: ignore\n",
    "\n",
    "    os.makedirs(os.path.dirname(cfg.out_daily_csv) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(cfg.out_weights_csv) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(cfg.out_json) or \".\", exist_ok=True)\n",
    "\n",
    "    daily.to_csv(cfg.out_daily_csv)\n",
    "    # Bundle weights + diagnostics in one CSV\n",
    "    weights_out = pd.concat([weights, es, exceed, methods], axis=1)\n",
    "    weights_out.to_csv(cfg.out_weights_csv)\n",
    "\n",
    "    with open(cfg.out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Saved daily → {cfg.out_daily_csv}\")\n",
    "    print(f\"[OK] Saved weights/EVT diagnostics → {cfg.out_weights_csv}\")\n",
    "    print(f\"[OK] Saved summary → {cfg.out_json}\")\n",
    "\n",
    "    pg = summary[\"performance_gross\"]\n",
    "    pn = summary[\"performance_net\"]\n",
    "    print(f\"[PERF gross] AnnRet={pg['ann_ret']:.2%} AnnVol={pg['ann_vol']:.2%} Sharpe={pg['sharpe']:.2f} MaxDD={pg['max_dd']:.2%}\")\n",
    "    print(f\"[PERF net  ] AnnRet={pn['ann_ret']:.2%} AnnVol={pn['ann_vol']:.2%} Sharpe={pn['sharpe']:.2f} MaxDD={pn['max_dd']:.2%}\")\n",
    "    t = summary[\"turnover\"]\n",
    "    print(f\"[TURN] Avg L1 turnover/rebal={t['avg_rebalance_turnover_L1']:.3f}  Median={t['median_rebalance_turnover_L1']:.3f}\")\n",
    "\n",
    "\n",
    "# ----------------------------- CLI -----------------------------\n",
    "def parse_args() -> Config:\n",
    "    p = argparse.ArgumentParser(description=\"Level-100: EVT ES Risk Budgeting (POT-GPD) + Monthly Rebalance + TC\")\n",
    "\n",
    "    p.add_argument(\"--start\", type=str, default=Config.start)\n",
    "    p.add_argument(\"--symbols\", nargs=\"+\", default=list(Config.symbols))\n",
    "\n",
    "    p.add_argument(\"--alpha\", type=float, default=Config.alpha)\n",
    "    p.add_argument(\"--q_u\", type=float, default=Config.q_u)\n",
    "    p.add_argument(\"--window\", type=int, default=Config.window)\n",
    "    p.add_argument(\"--min-exc\", dest=\"min_exc\", type=int, default=Config.min_exc)\n",
    "\n",
    "    p.add_argument(\"--rebalance\", type=str, default=Config.rebalance)  # use ME not M\n",
    "    p.add_argument(\"--max-w\", dest=\"max_w\", type=float, default=Config.max_w)\n",
    "    p.add_argument(\"--min-w\", dest=\"min_w\", type=float, default=Config.min_w)\n",
    "\n",
    "    p.add_argument(\"--no-corr-scale\", action=\"store_true\")\n",
    "    p.add_argument(\"--corr-shrink\", type=float, default=Config.corr_shrink)\n",
    "\n",
    "    p.add_argument(\"--tc_bps\", type=float, default=Config.tc_bps)\n",
    "\n",
    "    p.add_argument(\"--simple-returns\", action=\"store_true\")\n",
    "    p.add_argument(\"--no-dropna\", action=\"store_true\")\n",
    "    p.add_argument(\"--seed\", type=int, default=Config.seed)\n",
    "\n",
    "    p.add_argument(\"--daily-csv\", type=str, default=Config.out_daily_csv)\n",
    "    p.add_argument(\"--weights-csv\", type=str, default=Config.out_weights_csv)\n",
    "    p.add_argument(\"--json\", type=str, default=Config.out_json)\n",
    "\n",
    "    a = p.parse_args()\n",
    "\n",
    "    return Config(\n",
    "        symbols=tuple(a.symbols),\n",
    "        start=a.start,\n",
    "        alpha=float(a.alpha),\n",
    "        q_u=float(a.q_u),\n",
    "        window=int(a.window),\n",
    "        min_exc=int(a.min_exc),\n",
    "        rebalance=str(a.rebalance),\n",
    "        max_w=float(a.max_w),\n",
    "        min_w=float(a.min_w),\n",
    "        use_corr_scale=(not a.no_corr_scale),\n",
    "        corr_shrink=float(a.corr_shrink),\n",
    "        tc_bps=float(a.tc_bps),\n",
    "        use_log_returns=(not a.simple_returns),\n",
    "        dropna=(not a.no_dropna),\n",
    "        seed=int(a.seed),\n",
    "        out_daily_csv=a.daily_csv,\n",
    "        out_weights_csv=a.weights_csv,\n",
    "        out_json=a.json,\n",
    "    )\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    cfg = parse_args()\n",
    "    res = run_pipeline(cfg)\n",
    "    save_outputs(res, cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter/PyCharm shim\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]] + [\n",
    "        arg for arg in sys.argv[1:]\n",
    "        if arg != \"-f\" and not (arg.endswith(\".json\") and \"kernel\" in arg)\n",
    "    ]\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading prices for ('SPY', 'QQQ', 'IWM', 'EFA', 'EEM', 'TLT', 'LQD', 'GLD') from 2010-01-01 ...\n",
      "[INFO] rows=4020, assets=8, rebalances=145 (ME), window=1000\n",
      "[INFO] EVT alpha=0.05, q_u=0.9, min_exc=40, tc_bps=1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4020,8) (4063,8) ",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 554\u001B[39m\n\u001B[32m    549\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msys\u001B[39;00m\n\u001B[32m    550\u001B[39m sys.argv = [sys.argv[\u001B[32m0\u001B[39m]] + [\n\u001B[32m    551\u001B[39m     arg \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m sys.argv[\u001B[32m1\u001B[39m:]\n\u001B[32m    552\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m arg != \u001B[33m\"\u001B[39m\u001B[33m-f\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (arg.endswith(\u001B[33m\"\u001B[39m\u001B[33m.json\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mkernel\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m arg)\n\u001B[32m    553\u001B[39m ]\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 543\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    541\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmain\u001B[39m() -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    542\u001B[39m     cfg = parse_args()\n\u001B[32m--> \u001B[39m\u001B[32m543\u001B[39m     res = \u001B[43mrun_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    544\u001B[39m     save_outputs(res, cfg)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 374\u001B[39m, in \u001B[36mrun_pipeline\u001B[39m\u001B[34m(cfg)\u001B[39m\n\u001B[32m    371\u001B[39m w_daily.loc[:first_valid, :] = w_daily.loc[first_valid, :].values\n\u001B[32m    373\u001B[39m \u001B[38;5;66;03m# Compute daily portfolio returns with transaction costs at rebalance points\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m374\u001B[39m port = np.sum(\u001B[43mrets_np\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[43mw_daily\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m, axis=\u001B[32m1\u001B[39m)\n\u001B[32m    376\u001B[39m \u001B[38;5;66;03m# Turnover at rebalance dates (L1)\u001B[39;00m\n\u001B[32m    377\u001B[39m rebal_mask = np.zeros(n, dtype=\u001B[38;5;28mbool\u001B[39m)\n",
      "\u001B[31mValueError\u001B[39m: operands could not be broadcast together with shapes (4020,8) (4063,8) "
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
