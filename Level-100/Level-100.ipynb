{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-06T03:58:15.590268Z",
     "start_time": "2026-01-06T03:58:08.415995Z"
    }
   },
   "source": [
    "# level100_evt_es_risk_budget.py\n",
    "# Level-100: End-to-End Risk Engine — EVT (POT-GPD) Expected Shortfall Risk Budgeting\n",
    "#            + Monthly Rebalancing (TRADING DAY) + Turnover/TC + Walk-Forward Backtest (SciPy-free)\n",
    "#\n",
    "# Key repairs vs your version:\n",
    "# - Rebalance dates are now ALWAYS actual trading dates from rets.index (no calendar month-end rows injected).\n",
    "# - w_daily never expands beyond rets.index, so no broadcasting mismatch.\n",
    "# - Rebalance mask uses indexer (safe) instead of searchsorted on possibly-nonexistent dates.\n",
    "# - More robust yfinance close extraction + alignment + cleaning.\n",
    "# - Safer POT logic and clearer diagnostics.\n",
    "#\n",
    "# Run:\n",
    "#   python level100_evt_es_risk_budget.py\n",
    "#   python level100_evt_es_risk_budget.py --alpha 0.01 --q_u 0.95 --window 1260 --rebalance ME\n",
    "#   python level100_evt_es_risk_budget.py --symbols SPY QQQ IWM EFA EEM TLT LQD GLD --tc_bps 2.0\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "# ----------------------------- Config -----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    symbols: Tuple[str, ...] = (\"SPY\", \"QQQ\", \"IWM\", \"EFA\", \"EEM\", \"TLT\", \"LQD\", \"GLD\")\n",
    "    start: str = \"2010-01-01\"\n",
    "\n",
    "    use_log_returns: bool = True\n",
    "    dropna: bool = True\n",
    "\n",
    "    # EVT tail settings (on losses)\n",
    "    alpha: float = 0.05          # tail probability (ES at alpha)\n",
    "    q_u: float = 0.90            # threshold quantile for POT on losses (0.90 to 0.95 common)\n",
    "    window: int = 1000           # rolling window length (trading days)\n",
    "    min_exc: int = 40            # minimum exceedances required for POT fit\n",
    "    xi_clip: Tuple[float, float] = (-0.45, 0.45)  # stabilize MoM\n",
    "\n",
    "    # Rebalancing\n",
    "    rebalance: str = \"ME\"        # pandas offset alias; \"ME\" month-end bucket, \"W-FRI\", etc.\n",
    "    max_w: float = 0.35          # per-asset cap\n",
    "    min_w: float = 0.00          # per-asset floor (0 for long-only)\n",
    "    allow_cash: bool = False     # if True, residual weight goes to CASH (0% return)\n",
    "    cash_symbol: str = \"CASH\"\n",
    "\n",
    "    # Correlation diagnostics (optional; no leverage applied, we just compute a hint)\n",
    "    use_corr_scale: bool = True\n",
    "    corr_shrink: float = 0.15    # shrink corr toward identity to reduce noise\n",
    "\n",
    "    # Transaction costs\n",
    "    tc_bps: float = 1.0          # cost per 100% turnover at rebalance (bps)\n",
    "                                 # applied: cost = tc_bps/1e4 * turnover\n",
    "\n",
    "    seed: int = 42\n",
    "\n",
    "    out_daily_csv: str = \"level100_evt_es_rb_daily.csv\"\n",
    "    out_weights_csv: str = \"level100_evt_es_rb_weights.csv\"\n",
    "    out_json: str = \"level100_evt_es_rb_summary.json\"\n",
    "\n",
    "\n",
    "# ----------------------------- yfinance loader -----------------------------\n",
    "def _extract_close(px: pd.DataFrame, symbol: str) -> pd.Series:\n",
    "    if px is None or px.empty:\n",
    "        raise RuntimeError(f\"No data returned for {symbol}\")\n",
    "\n",
    "    # yfinance: MultiIndex columns often like ('Adj Close','SPY') or ('Close','SPY')\n",
    "    if isinstance(px.columns, pd.MultiIndex):\n",
    "        # Try common layouts\n",
    "        candidates = [\n",
    "            (\"Adj Close\", symbol),\n",
    "            (\"Close\", symbol),\n",
    "            (symbol, \"Adj Close\"),\n",
    "            (symbol, \"Close\"),\n",
    "        ]\n",
    "        for key in candidates:\n",
    "            if key in px.columns:\n",
    "                s = px[key]\n",
    "                if isinstance(s, pd.DataFrame):\n",
    "                    s = s.iloc[:, 0]\n",
    "                s = s.copy()\n",
    "                s.name = symbol\n",
    "                return s\n",
    "        raise RuntimeError(f\"Could not extract Close/Adj Close for {symbol} from MultiIndex columns.\")\n",
    "\n",
    "    # Single-index columns: may contain Adj Close/Close\n",
    "    for col in (\"Adj Close\", \"Close\"):\n",
    "        if col in px.columns:\n",
    "            s = px[col]\n",
    "            if isinstance(s, pd.DataFrame):\n",
    "                s = s.iloc[:, 0]\n",
    "            s = s.copy()\n",
    "            s.name = symbol\n",
    "            return s\n",
    "\n",
    "    raise RuntimeError(f\"Missing Close/Adj Close for {symbol}. Columns={list(px.columns)}\")\n",
    "\n",
    "\n",
    "def load_prices(symbols: Tuple[str, ...], start: str) -> pd.DataFrame:\n",
    "    syms = tuple(symbols)\n",
    "\n",
    "    # Batch download\n",
    "    try:\n",
    "        px_all = yf.download(list(syms), start=start, progress=False, group_by=\"column\", auto_adjust=False)\n",
    "        if px_all is not None and not px_all.empty:\n",
    "            series = []\n",
    "            for s in syms:\n",
    "                series.append(_extract_close(px_all, s))\n",
    "            out = pd.concat(series, axis=1).sort_index()\n",
    "            return out\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback single-ticker downloads\n",
    "    series = []\n",
    "    for s in syms:\n",
    "        px = yf.download(s, start=start, progress=False, auto_adjust=False)\n",
    "        series.append(_extract_close(px, s))\n",
    "    return pd.concat(series, axis=1).sort_index()\n",
    "\n",
    "\n",
    "def compute_returns(prices: pd.DataFrame, use_log: bool) -> pd.DataFrame:\n",
    "    prices = prices.replace([np.inf, -np.inf], np.nan)\n",
    "    if use_log:\n",
    "        rets = np.log(prices).diff()\n",
    "    else:\n",
    "        rets = prices.pct_change()\n",
    "    rets = rets.replace([np.inf, -np.inf], np.nan)\n",
    "    rets = rets.dropna(how=\"all\")\n",
    "    return rets\n",
    "\n",
    "\n",
    "# ----------------------------- EVT POT-GPD (fast MoM) -----------------------------\n",
    "def gpd_mom_fit(exceed: np.ndarray, xi_clip: Tuple[float, float]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Method-of-moments fit for GPD on exceedances y>0.\n",
    "      mean = beta/(1-xi)\n",
    "      var  = beta^2/((1-xi)^2*(1-2xi))\n",
    "    => var/mean^2 = 1/(1-2xi)  => xi = 0.5*(1 - mean^2/var)\n",
    "       beta = mean*(1-xi)\n",
    "    \"\"\"\n",
    "    y = exceed.astype(float)\n",
    "    y = y[np.isfinite(y)]\n",
    "    if y.size == 0:\n",
    "        return 0.0, 1e-12\n",
    "\n",
    "    m = float(y.mean())\n",
    "    v = float(y.var(ddof=1)) if y.size >= 2 else float(y.var())\n",
    "    v = max(v, 1e-12)\n",
    "\n",
    "    xi = 0.5 * (1.0 - (m * m) / v)\n",
    "    xi = float(np.clip(xi, xi_clip[0], xi_clip[1]))\n",
    "\n",
    "    beta = m * (1.0 - xi)\n",
    "    beta = float(max(beta, 1e-12))\n",
    "    return xi, beta\n",
    "\n",
    "\n",
    "def pot_es_loss(\n",
    "    losses: np.ndarray,\n",
    "    alpha: float,\n",
    "    q_u: float,\n",
    "    min_exc: int,\n",
    "    xi_clip: Tuple[float, float],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Estimate ES on LOSSES using POT-GPD. Losses should be positive-ish for bad days.\n",
    "    Returns dict:\n",
    "      es_loss, var_loss, u, k, method\n",
    "    Falls back to empirical ES if POT invalid/insufficient.\n",
    "    \"\"\"\n",
    "    L = losses.astype(float)\n",
    "    L = L[np.isfinite(L)]\n",
    "    n = int(L.size)\n",
    "    if n < 20:\n",
    "        return {\"es_loss\": float(\"nan\"), \"var_loss\": float(\"nan\"), \"u\": float(\"nan\"), \"k\": 0.0, \"method\": \"na\"}\n",
    "\n",
    "    # Empirical right-tail VaR/ES on losses:\n",
    "    # P(L > VaR) = alpha  => VaR = quantile at (1-alpha)\n",
    "    var_emp = float(np.quantile(L, 1.0 - alpha))\n",
    "    tail = L[L >= var_emp]\n",
    "    es_emp = float(tail.mean()) if tail.size > 0 else float(\"nan\")\n",
    "\n",
    "    # POT threshold\n",
    "    u = float(np.quantile(L, q_u))\n",
    "    exc = L[L > u] - u\n",
    "    k = int(exc.size)\n",
    "\n",
    "    # Not enough exceedances\n",
    "    if k < min_exc:\n",
    "        return {\"es_loss\": es_emp, \"var_loss\": var_emp, \"u\": u, \"k\": float(k), \"method\": \"empirical\"}\n",
    "\n",
    "    p_u = k / n  # P(L > u)\n",
    "\n",
    "    # If alpha is not deeper than u's exceedance probability, POT extrapolation isn't needed/valid\n",
    "    # Need alpha < p_u for VaR beyond u.\n",
    "    if not (alpha < p_u):\n",
    "        return {\"es_loss\": es_emp, \"var_loss\": var_emp, \"u\": u, \"k\": float(k), \"method\": \"empirical\"}\n",
    "\n",
    "    xi, beta = gpd_mom_fit(exc, xi_clip=xi_clip)\n",
    "\n",
    "    # POT VaR for losses where P(L > VaR)=alpha\n",
    "    # VaR = u + (beta/xi)*((alpha/p_u)^(-xi) - 1) ; limit xi->0 uses log\n",
    "    if abs(xi) < 1e-8:\n",
    "        var_pot = u + beta * math.log(p_u / alpha)\n",
    "    else:\n",
    "        var_pot = u + (beta / xi) * (((alpha / p_u) ** (-xi)) - 1.0)\n",
    "\n",
    "    # POT ES for losses (finite if xi < 1)\n",
    "    if xi >= 0.999:\n",
    "        return {\"es_loss\": es_emp, \"var_loss\": var_emp, \"u\": u, \"k\": float(k), \"method\": \"empirical\"}\n",
    "\n",
    "    yq = var_pot - u\n",
    "    es_pot = var_pot + (beta + xi * yq) / (1.0 - xi)\n",
    "\n",
    "    # Guardrails\n",
    "    if not np.isfinite(es_pot) or es_pot <= 0:\n",
    "        return {\"es_loss\": es_emp, \"var_loss\": var_emp, \"u\": u, \"k\": float(k), \"method\": \"empirical\"}\n",
    "\n",
    "    return {\"es_loss\": float(es_pot), \"var_loss\": float(var_pot), \"u\": u, \"k\": float(k), \"method\": \"pot\"}\n",
    "\n",
    "\n",
    "# ----------------------------- Portfolio mechanics -----------------------------\n",
    "def shrink_corr(corr: np.ndarray, shrink: float) -> np.ndarray:\n",
    "    n = corr.shape[0]\n",
    "    s = float(np.clip(shrink, 0.0, 1.0))\n",
    "    out = (1.0 - s) * corr + s * np.eye(n)\n",
    "    np.fill_diagonal(out, 1.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def corr_scale_for_weights(w: np.ndarray, corr: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Scalar diagnostic: 1/sqrt(w' C w) where C is correlation.\n",
    "    NOT applied as leverage here; just reported as leverage_hint.\n",
    "    \"\"\"\n",
    "    v = float(w @ corr @ w)\n",
    "    v = max(v, 1e-12)\n",
    "    return 1.0 / math.sqrt(v)\n",
    "\n",
    "\n",
    "def clip_and_renorm_longonly(w: np.ndarray, lo: float, hi: float) -> np.ndarray:\n",
    "    w = np.clip(w, lo, hi)\n",
    "    s = float(w.sum())\n",
    "    if s <= 0:\n",
    "        return np.ones_like(w) / len(w)\n",
    "    return w / s\n",
    "\n",
    "\n",
    "def perf_stats(r: np.ndarray) -> Dict[str, float]:\n",
    "    r = r[np.isfinite(r)]\n",
    "    if r.size == 0:\n",
    "        return {\"ann_ret\": float(\"nan\"), \"ann_vol\": float(\"nan\"), \"sharpe\": float(\"nan\"), \"max_dd\": float(\"nan\")}\n",
    "    ann_ret = float(r.mean() * 252.0)\n",
    "    ann_vol = float(r.std(ddof=1) * math.sqrt(252.0))\n",
    "    sharpe = float(ann_ret / ann_vol) if ann_vol > 0 else float(\"nan\")\n",
    "    eq = np.cumprod(1.0 + r)\n",
    "    peak = np.maximum.accumulate(eq)\n",
    "    dd = (eq / peak) - 1.0\n",
    "    max_dd = float(dd.min()) if dd.size else float(\"nan\")\n",
    "    return {\"ann_ret\": ann_ret, \"ann_vol\": ann_vol, \"sharpe\": sharpe, \"max_dd\": max_dd}\n",
    "\n",
    "\n",
    "def trading_rebalance_dates(index: pd.DatetimeIndex, freq: str) -> pd.DatetimeIndex:\n",
    "    \"\"\"\n",
    "    Build rebalance dates that are ALWAYS actual dates in `index`.\n",
    "    We bucket by freq and take the LAST trading day in each bucket.\n",
    "    \"\"\"\n",
    "    if len(index) == 0:\n",
    "        return pd.DatetimeIndex([])\n",
    "\n",
    "    # groupby with Grouper creates buckets labelled by period end, but we take df.index[-1]\n",
    "    s = pd.Series(index=index, data=np.ones(len(index)))\n",
    "    # groupby on the index itself\n",
    "    grouped = s.groupby(pd.Grouper(freq=freq))\n",
    "    last_dates = grouped.apply(lambda x: x.index[-1] if len(x) else pd.NaT).dropna()\n",
    "    return pd.DatetimeIndex(last_dates.values)\n",
    "\n",
    "\n",
    "# ----------------------------- Pipeline -----------------------------\n",
    "def run_pipeline(cfg: Config) -> Dict[str, Any]:\n",
    "    np.random.seed(cfg.seed)\n",
    "\n",
    "    print(f\"[INFO] Downloading prices for {cfg.symbols} from {cfg.start} ...\")\n",
    "    prices = load_prices(cfg.symbols, cfg.start)\n",
    "\n",
    "    # Basic cleaning\n",
    "    prices = prices.replace([np.inf, -np.inf], np.nan).dropna(how=\"all\")\n",
    "    if prices.empty:\n",
    "        raise RuntimeError(\"No prices after cleaning (check tickers/start).\")\n",
    "\n",
    "    rets = compute_returns(prices, cfg.use_log_returns)\n",
    "\n",
    "    if cfg.dropna:\n",
    "        rets = rets.dropna(how=\"any\")\n",
    "    if rets.empty:\n",
    "        raise RuntimeError(\"No returns after cleaning (check tickers/start).\")\n",
    "\n",
    "    dates = rets.index\n",
    "    n, m = rets.shape\n",
    "    if n <= cfg.window + 10:\n",
    "        raise RuntimeError(f\"Not enough rows for window={cfg.window}. rows={n}\")\n",
    "\n",
    "    # Rebalance dates must be trading dates\n",
    "    rebal_dates = trading_rebalance_dates(dates, cfg.rebalance)\n",
    "    # Start once we have enough lookback\n",
    "    rebal_dates = rebal_dates[rebal_dates >= dates[cfg.window]]\n",
    "\n",
    "    if len(rebal_dates) == 0:\n",
    "        raise RuntimeError(\"No rebalance dates found after applying window constraint.\")\n",
    "\n",
    "    print(f\"[INFO] rows={n}, assets={m}, rebalances={len(rebal_dates)} ({cfg.rebalance}), window={cfg.window}\")\n",
    "    print(f\"[INFO] EVT alpha={cfg.alpha}, q_u={cfg.q_u}, min_exc={cfg.min_exc}, tc_bps={cfg.tc_bps}\")\n",
    "\n",
    "    syms = list(cfg.symbols)\n",
    "    rets_np = rets.values\n",
    "\n",
    "    # Storage: rebalance outputs\n",
    "    w_hist = pd.DataFrame(index=rebal_dates, columns=syms, dtype=float)\n",
    "    lev_hint = pd.Series(index=rebal_dates, dtype=float, name=\"leverage_hint\")\n",
    "\n",
    "    es_hist = pd.DataFrame(index=rebal_dates, columns=[f\"ESloss_{s}\" for s in syms], dtype=float)\n",
    "    method_hist = pd.DataFrame(index=rebal_dates, columns=[f\"method_{s}\" for s in syms], dtype=str)\n",
    "    exc_hist = pd.DataFrame(index=rebal_dates, columns=[f\"k_{s}\" for s in syms], dtype=float)\n",
    "\n",
    "    # Daily weights aligned EXACTLY to returns index\n",
    "    w_daily = pd.DataFrame(index=dates, columns=syms, dtype=float)\n",
    "\n",
    "    # Initial weights\n",
    "    prev_w = np.ones(m, dtype=float) / m\n",
    "\n",
    "    # Precompute mapping dt -> integer location in returns\n",
    "    rebal_locs = dates.get_indexer(rebal_dates)\n",
    "    if np.any(rebal_locs < 0):\n",
    "        # should never happen since trading_rebalance_dates guarantees membership\n",
    "        raise RuntimeError(\"Internal error: some rebalance dates not found in returns index.\")\n",
    "\n",
    "    for dt, t in zip(rebal_dates, rebal_locs):\n",
    "        if t <= cfg.window:\n",
    "            continue\n",
    "\n",
    "        win = rets_np[t - cfg.window:t, :]  # (window, assets)\n",
    "        losses = -win  # right tail = big losses\n",
    "\n",
    "        es = np.full(m, np.nan, dtype=float)\n",
    "\n",
    "        for j, s in enumerate(syms):\n",
    "            res = pot_es_loss(\n",
    "                losses=losses[:, j],\n",
    "                alpha=cfg.alpha,\n",
    "                q_u=cfg.q_u,\n",
    "                min_exc=cfg.min_exc,\n",
    "                xi_clip=cfg.xi_clip,\n",
    "            )\n",
    "            es[j] = res[\"es_loss\"]\n",
    "            es_hist.loc[dt, f\"ESloss_{s}\"] = res[\"es_loss\"]\n",
    "            method_hist.loc[dt, f\"method_{s}\"] = res[\"method\"]\n",
    "            exc_hist.loc[dt, f\"k_{s}\"] = res[\"k\"]\n",
    "\n",
    "        # Risk budgeting: w ∝ 1 / ES_loss\n",
    "        es_ok = np.isfinite(es) & (es > 1e-12)\n",
    "        if not np.any(es_ok):\n",
    "            raw = np.ones(m, dtype=float) / m\n",
    "        else:\n",
    "            inv = np.zeros(m, dtype=float)\n",
    "            inv[es_ok] = 1.0 / es[es_ok]\n",
    "            s_inv = float(inv.sum())\n",
    "            raw = inv / s_inv if s_inv > 0 else (np.ones(m, dtype=float) / m)\n",
    "\n",
    "        # Caps/floors (long-only)\n",
    "        w = clip_and_renorm_longonly(raw, cfg.min_w, cfg.max_w)\n",
    "\n",
    "        # Corr-based diagnostic leverage hint\n",
    "        if cfg.use_corr_scale:\n",
    "            C = np.corrcoef(win, rowvar=False)\n",
    "            C = np.nan_to_num(C, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            C = shrink_corr(C, cfg.corr_shrink)\n",
    "            lev_hint.loc[dt] = corr_scale_for_weights(w, C)\n",
    "        else:\n",
    "            lev_hint.loc[dt] = float(\"nan\")\n",
    "\n",
    "        # Store\n",
    "        w_hist.loc[dt, :] = w\n",
    "        w_daily.iloc[t, :] = w  # IMPORTANT: writes by integer location => never expands index\n",
    "        prev_w = w\n",
    "\n",
    "    # Forward-fill weights over trading days\n",
    "    w_daily = w_daily.ffill()\n",
    "\n",
    "    # Backfill leading NaNs with first computed weights\n",
    "    first_valid = w_daily.dropna(how=\"any\").index.min()\n",
    "    if first_valid is None:\n",
    "        raise RuntimeError(\"No weights computed. Try smaller window or lower min_exc or different rebalance freq.\")\n",
    "\n",
    "    w_daily.loc[:first_valid, :] = w_daily.loc[first_valid, :].values\n",
    "\n",
    "    # Portfolio gross returns\n",
    "    port_gross = np.sum(rets_np * w_daily.values, axis=1)\n",
    "\n",
    "    # Rebalance mask (on trading dates)\n",
    "    rebal_mask = np.zeros(n, dtype=bool)\n",
    "    rebal_mask[rebal_locs] = True\n",
    "\n",
    "    # Turnover and TC\n",
    "    w_vals = w_daily.values\n",
    "    w_prev = np.vstack([w_vals[0:1, :], w_vals[:-1, :]])\n",
    "    turnover = np.sum(np.abs(w_vals - w_prev), axis=1)\n",
    "    tc = (cfg.tc_bps / 1e4) * turnover * rebal_mask.astype(float)\n",
    "\n",
    "    port_net = port_gross - tc\n",
    "\n",
    "    # Daily output\n",
    "    daily = pd.DataFrame(index=dates)\n",
    "    daily[\"port_ret_gross\"] = port_gross\n",
    "    daily[\"port_ret_net\"] = port_net\n",
    "    daily[\"turnover\"] = turnover\n",
    "    daily[\"tc_cost\"] = tc\n",
    "    for j, s in enumerate(syms):\n",
    "        daily[f\"w_{s}\"] = w_daily.iloc[:, j].values\n",
    "        daily[f\"ret_{s}\"] = rets.iloc[:, j].values\n",
    "\n",
    "    # Summary stats\n",
    "    stats_gross = perf_stats(daily[\"port_ret_gross\"].values)\n",
    "    stats_net = perf_stats(daily[\"port_ret_net\"].values)\n",
    "\n",
    "    avg_turn = float(np.mean(turnover[rebal_mask])) if rebal_mask.any() else float(\"nan\")\n",
    "    med_turn = float(np.median(turnover[rebal_mask])) if rebal_mask.any() else float(\"nan\")\n",
    "\n",
    "    # Quick rolling empirical VaR exceptions on net returns (sanity)\n",
    "    r_net = daily[\"port_ret_net\"].values\n",
    "    var_roll = np.full(n, np.nan)\n",
    "    for t in range(cfg.window, n):\n",
    "        winp = r_net[t - cfg.window:t]\n",
    "        if np.isfinite(winp).sum() > 50:\n",
    "            var_roll[t] = float(np.quantile(winp[np.isfinite(winp)], cfg.alpha))\n",
    "    exc = np.isfinite(var_roll) & (r_net <= var_roll)\n",
    "    denom = int(np.isfinite(var_roll).sum())\n",
    "    exc_rate = float(exc.sum() / denom) if denom > 0 else float(\"nan\")\n",
    "\n",
    "    summary = {\n",
    "        \"config\": asdict(cfg),\n",
    "        \"data_window\": {\"start\": str(dates.min().date()), \"end\": str(dates.max().date()), \"n_returns\": int(n)},\n",
    "        \"rebalancing\": {\"freq\": cfg.rebalance, \"n_rebalances\": int(len(rebal_dates))},\n",
    "        \"performance_gross\": stats_gross,\n",
    "        \"performance_net\": stats_net,\n",
    "        \"turnover\": {\"avg_rebalance_turnover_L1\": avg_turn, \"median_rebalance_turnover_L1\": med_turn},\n",
    "        \"quick_var_backtest\": {\n",
    "            \"alpha\": float(cfg.alpha),\n",
    "            \"exception_rate_vs_empirical_rolling_var\": exc_rate,\n",
    "            \"note\": \"Quick sanity check using rolling empirical VaR on portfolio net returns.\"\n",
    "        },\n",
    "        \"notes\": [\n",
    "            \"Weights are long-only and sum to 1 (no leverage).\",\n",
    "            \"Tail-risk model uses EVT POT-GPD ES on losses per asset; falls back to empirical ES when exceedances are too few.\",\n",
    "            \"Rebalance dates are mapped to actual trading days (prevents index expansion bugs).\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Weights output with diagnostics bundled\n",
    "    weights_out = w_hist.copy()\n",
    "    weights_out[\"leverage_hint\"] = lev_hint\n",
    "\n",
    "    diagnostics = pd.concat([es_hist, exc_hist, method_hist], axis=1)\n",
    "    weights_out_full = pd.concat([weights_out, diagnostics], axis=1)\n",
    "\n",
    "    return {\n",
    "        \"daily\": daily,\n",
    "        \"weights\": weights_out_full,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_outputs(res: Dict[str, Any], cfg: Config) -> None:\n",
    "    daily: pd.DataFrame = res[\"daily\"]\n",
    "    weights: pd.DataFrame = res[\"weights\"]\n",
    "    summary: Dict[str, Any] = res[\"summary\"]\n",
    "\n",
    "    os.makedirs(os.path.dirname(cfg.out_daily_csv) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(cfg.out_weights_csv) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(cfg.out_json) or \".\", exist_ok=True)\n",
    "\n",
    "    daily.to_csv(cfg.out_daily_csv)\n",
    "    weights.to_csv(cfg.out_weights_csv)\n",
    "\n",
    "    with open(cfg.out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Saved daily → {cfg.out_daily_csv}\")\n",
    "    print(f\"[OK] Saved weights/EVT diagnostics → {cfg.out_weights_csv}\")\n",
    "    print(f\"[OK] Saved summary → {cfg.out_json}\")\n",
    "\n",
    "    pg = summary[\"performance_gross\"]\n",
    "    pn = summary[\"performance_net\"]\n",
    "    print(f\"[PERF gross] AnnRet={pg['ann_ret']:.2%} AnnVol={pg['ann_vol']:.2%} Sharpe={pg['sharpe']:.2f} MaxDD={pg['max_dd']:.2%}\")\n",
    "    print(f\"[PERF net  ] AnnRet={pn['ann_ret']:.2%} AnnVol={pn['ann_vol']:.2%} Sharpe={pn['sharpe']:.2f} MaxDD={pn['max_dd']:.2%}\")\n",
    "    t = summary[\"turnover\"]\n",
    "    print(f\"[TURN] Avg L1 turnover/rebal={t['avg_rebalance_turnover_L1']:.3f}  Median={t['median_rebalance_turnover_L1']:.3f}\")\n",
    "\n",
    "\n",
    "# ----------------------------- CLI -----------------------------\n",
    "def parse_args() -> Config:\n",
    "    p = argparse.ArgumentParser(description=\"Level-100: EVT ES Risk Budgeting (POT-GPD) + Rebalance + TC (SciPy-free)\")\n",
    "\n",
    "    p.add_argument(\"--start\", type=str, default=Config.start)\n",
    "    p.add_argument(\"--symbols\", nargs=\"+\", default=list(Config.symbols))\n",
    "\n",
    "    p.add_argument(\"--alpha\", type=float, default=Config.alpha)\n",
    "    p.add_argument(\"--q_u\", type=float, default=Config.q_u)\n",
    "    p.add_argument(\"--window\", type=int, default=Config.window)\n",
    "    p.add_argument(\"--min-exc\", dest=\"min_exc\", type=int, default=Config.min_exc)\n",
    "\n",
    "    p.add_argument(\"--rebalance\", type=str, default=Config.rebalance)\n",
    "    p.add_argument(\"--max-w\", dest=\"max_w\", type=float, default=Config.max_w)\n",
    "    p.add_argument(\"--min-w\", dest=\"min_w\", type=float, default=Config.min_w)\n",
    "\n",
    "    p.add_argument(\"--no-corr-scale\", action=\"store_true\")\n",
    "    p.add_argument(\"--corr-shrink\", type=float, default=Config.corr_shrink)\n",
    "\n",
    "    p.add_argument(\"--tc_bps\", type=float, default=Config.tc_bps)\n",
    "\n",
    "    p.add_argument(\"--simple-returns\", action=\"store_true\")\n",
    "    p.add_argument(\"--no-dropna\", action=\"store_true\")\n",
    "    p.add_argument(\"--seed\", type=int, default=Config.seed)\n",
    "\n",
    "    p.add_argument(\"--daily-csv\", type=str, default=Config.out_daily_csv)\n",
    "    p.add_argument(\"--weights-csv\", type=str, default=Config.out_weights_csv)\n",
    "    p.add_argument(\"--json\", type=str, default=Config.out_json)\n",
    "\n",
    "    a = p.parse_args()\n",
    "\n",
    "    return Config(\n",
    "        symbols=tuple(a.symbols),\n",
    "        start=a.start,\n",
    "        alpha=float(a.alpha),\n",
    "        q_u=float(a.q_u),\n",
    "        window=int(a.window),\n",
    "        min_exc=int(a.min_exc),\n",
    "        rebalance=str(a.rebalance),\n",
    "        max_w=float(a.max_w),\n",
    "        min_w=float(a.min_w),\n",
    "        use_corr_scale=(not a.no_corr_scale),\n",
    "        corr_shrink=float(a.corr_shrink),\n",
    "        tc_bps=float(a.tc_bps),\n",
    "        use_log_returns=(not a.simple_returns),\n",
    "        dropna=(not a.no_dropna),\n",
    "        seed=int(a.seed),\n",
    "        out_daily_csv=a.daily_csv,\n",
    "        out_weights_csv=a.weights_csv,\n",
    "        out_json=a.json,\n",
    "    )\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    cfg = parse_args()\n",
    "    res = run_pipeline(cfg)\n",
    "    save_outputs(res, cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter/PyCharm shim: drop kernel args\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]] + [\n",
    "        arg for arg in sys.argv[1:]\n",
    "        if arg != \"-f\" and not (arg.endswith(\".json\") and \"kernel\" in arg)\n",
    "    ]\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading prices for ('SPY', 'QQQ', 'IWM', 'EFA', 'EEM', 'TLT', 'LQD', 'GLD') from 2010-01-01 ...\n",
      "[INFO] rows=4025, assets=8, rebalances=146 (ME), window=1000\n",
      "[INFO] EVT alpha=0.05, q_u=0.9, min_exc=40, tc_bps=1.0\n",
      "[OK] Saved daily → level100_evt_es_rb_daily.csv\n",
      "[OK] Saved weights/EVT diagnostics → level100_evt_es_rb_weights.csv\n",
      "[OK] Saved summary → level100_evt_es_rb_summary.json\n",
      "[PERF gross] AnnRet=7.42% AnnVol=9.63% Sharpe=0.77 MaxDD=-26.90%\n",
      "[PERF net  ] AnnRet=7.41% AnnVol=9.63% Sharpe=0.77 MaxDD=-26.90%\n",
      "[TURN] Avg L1 turnover/rebal=0.009  Median=0.006\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
